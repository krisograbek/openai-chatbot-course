{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "\n",
    "In this notebook, you'll learn about tokens - a crucial concept tied directly to Large Language Models (LLMs).\n",
    "\n",
    "\n",
    "Here's what you will learn:\n",
    "- What is a token?\n",
    "- How to count tokens?\n",
    "- Why LLMs can't count letters?\n",
    "- Why do we use sub-words (not characters or words)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ideas:\n",
    "- should I encode for gpt-4 and decode for gpt-4o (or vice versa)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n",
    "\n",
    "A token is a chunk of text that Large Language Models read or generate.\n",
    "\n",
    "Here's key information about tokens:\n",
    "- Tokens are atomic units that represent our language.\n",
    "- Tokens allow LLMs to efficiently understand language.\n",
    "- Tokens are the smallest unit of text that AI models process.\n",
    "- Tokens don't have the defined length. Some are only 1 character long, others can be longer words.\n",
    "- Tokens can be: words, sub-words, punctuation marks or special symbols.\n",
    "- As a rule of thumb, a token corresponds to 3/4 of the word. So 100 tokens is roughly 75 words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why sub-words (not characters or words) for tokens?\n",
    "\n",
    "#TODO: \n",
    "- Explain tokens are turned into embeddings inside of LLMs, as they have a lookup table in which they get IDs (tokens). The embedding values are trainable and get updated during the training of the model.\n",
    "\n",
    "For tokens, we use sub-words. Here's an example:\n",
    "\n",
    "<img src=\"./images/Sub-wordExample.png\" alt=\"Sub word token\" width=\"500\" />\n",
    "\n",
    "\n",
    "People often ask:\n",
    "- \"Why not characters?\"\n",
    "- \"Why not entire words?\"\n",
    "\n",
    "Let me explain...\n",
    "\n",
    "### The Cons of character-based approach.\n",
    "1. **Too little information**. Each character in itself holds very little semantic meaning.\n",
    "2. **Longer sequences**. The more tokens we pass, the higher computational power required. Character tokens create overly long sequences, which makes them extremely inefficient.\n",
    "3. **Complex training**. It's hard to understand word meanings, context and relationships using only characters.\n",
    "\n",
    "\n",
    "### The Cons of full word-based approach.\n",
    "1. **Large Token Vocabulary**. When every unique word becomes a token, we and up with an enormous vocabulary. It's inefficient in memory and processing resources.\n",
    "2. **Out-of-Vocabulary (OOV) Words**. The model can't handle rare or new words.\n",
    "3. **Misspelling and variation problems**. It's impossible to add all variations or misspelled words into model's vocabulary. Look at the above image. We'd need a separate token for words like: unlearn, unfit, reworked, redesigning, deconstructed. But we can perfectly capture their meaning using 2 tokens.\n",
    "\n",
    "### How sub-words combine the best of both words?\n",
    "Sub-words combine the best of both words, while decreasing the limitaions of them.\n",
    "1. **Smaller vocabulary**. Sub-words reduce vocabulary by using smaller and reusable chunks of words.\n",
    "2. **Handling OOV words.** When a model \"sees\" a word for the first time it can break it down into smaller chunks. And the model knows the chunks already, so it can capture the meaning of the new word.\n",
    "3. **Balance between length and information.** Optimization methods find the best sub-word combinations for a language.\n",
    "\n",
    "\n",
    "In summary:\n",
    "- Character tokens are too small and carry too little information (lack of semantic meaning).\n",
    "- Word tokens are too large and create huge vocabularies.\n",
    "- Combine more efficient token vocabularies with semantic representation.\n",
    "\n",
    "Here are more examples on where sub-word tokens shine.\n",
    "\n",
    "<img src=\"./images/LongWords.png\" alt=\"long words\" width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words, letters and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amazon Rainforest is the largest tropical rainforest in the world, spanning over 5.5 million square kilometers. It is home to an incredible diversity of wildlife, including thousands of plant species, birds, mammals, and insects. The rainforest plays a crucial role in regulating the Earth's climate and is often referred to as the \"lungs of the planet\" due to its vast capacity for carbon dioxide absorption.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "am_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write 3 short sentences about Amazon Rainforest\"}\n",
    "    ],\n",
    "    seed=42,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "am_response = am_completion.choices[0].message.content\n",
    "print(am_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy:\n",
    "\n",
    "\n",
    "```\n",
    "The Amazon Rainforest is the largest tropical rainforest in the world, spanning over 5.5 million square kilometers. It is home to an incredible diversity of wildlife, including thousands of plant species, birds, mammals, and insects. The rainforest plays a crucial role in regulating the Earth's climate and is often referred to as the \"lungs of the planet\" due to its vast capacity for carbon dioxide absorption.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've got a short description about The Amazon Rainforest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words and characters.\n",
    "\n",
    "Let's count words and characters first. In Python it's quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 66 words and 413 characters.\n"
     ]
    }
   ],
   "source": [
    "words_cnt = len(am_response.split())\n",
    "characters_cnt = len(am_response)\n",
    "\n",
    "print(f\"The response has {words_cnt} words and {characters_cnt} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so our Amazon Rainforest description has 66 words and 413 characters.\n",
    "\n",
    "Note: Your description may be different. So if your results differ, don't freak out :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 80 tokens.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "tokens = enc.encode(am_response)\n",
    "\n",
    "print(f\"The response has {len(tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break the code down:\n",
    "- We imported the tiktoken library.\n",
    "- We defined the encoder using `encoding_for_model(\"gpt-4o-mini\")` to ensure we use the right encoder. \n",
    "- We \"tokenized\" the response using `encode(response)`.\n",
    "- We counted the tokens using Python's `len` function.\n",
    "\n",
    "Great!\n",
    "\n",
    "Let's take our sample text and run it through the [online tokenizer](https://tiktokenizer.vercel.app/).\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "<img src=\"./images/AmazonDescTokens2.png\" alt=\"Amazon Description tokens\" width=\"500px\" />\n",
    "\n",
    "I love that visual representation. The app highlights every single token. It helps us see how they actually look like.\n",
    "\n",
    "Below, we can see the numerical representation of each token from the decription.\n",
    "\n",
    "Let's try to see, if the numbers match with the tokens from the `tiktoken` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[976, 9529, 33159, 76428, 382, 290, 10574, 40068, 164436, 306, 290, 2375, 11, 66335, 1072, 220, 20, 13, 20, 5749, 13749, 63677, 13, 1225, 382, 2237, 316, 448, 19201, 28955, 328, 40214, 11, 3463, 13369, 328, 6804, 15361, 11, 28510, 11, 119032, 11, 326, 65129, 13, 623, 164436, 17473, 261, 19008, 5430, 306, 101955, 290, 146677, 16721, 326, 382, 4783, 22653, 316, 472, 290, 392, 82576, 328, 290, 17921, 1, 5192, 316, 1617, 11332, 12241, 395, 15883, 70513, 57036, 13]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be crystal clear. We now compare the tokens from code with the numbers from the last image. They are identical because we used the same text with the same encoder in both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll count words, characters, and tokens later too.\n",
    "\n",
    "Let's write helper functions for counting and printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_words_and_chars(text):\n",
    "    words_cnt = len(text.split())\n",
    "    characters_cnt = len(text)\n",
    "\n",
    "    print(f\"The text has {words_cnt} words and {characters_cnt} characters.\")\n",
    "    \n",
    "def count_tokens(text):\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    tokens = enc.encode(text)\n",
    "\n",
    "    print(f\"The response has {len(tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking GPT-4o Mini to count words, characters, and tokens.\n",
    "\n",
    "Let's ask GPT-4o Mini to count for us.\n",
    "\n",
    "As a reminder, the correct answers for our description are:\n",
    "- 66 words\n",
    "- 413 characters\n",
    "- 80 tokens\n",
    "\n",
    "Good luck, GPT-4o!\n",
    "\n",
    "**Counting Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph contains 63 words.\n"
     ]
    }
   ],
   "source": [
    "word_prompt = \"How many words is in the following paragraph: \" + am_response\n",
    "\n",
    "words = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": word_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "word_response = words.choices[0].message.content\n",
    "print(word_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph you provided contains 469 characters, including spaces and punctuation.\n"
     ]
    }
   ],
   "source": [
    "character_prompt = \"How many characters is in the following paragraph: \" + am_response\n",
    "\n",
    "characters = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": character_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "character_response = characters.choices[0].message.content\n",
    "print(character_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize:\n",
    "- The word count was close (63 instead of 66).\n",
    "- The character count was bad (469 instead of 413).\n",
    "\n",
    "\n",
    "What about tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of tokens in the provided paragraph, we can break it down into individual words and punctuation marks. In natural language processing, a token typically refers to a word or a punctuation mark.\n",
      "\n",
      "The paragraph you provided contains the following text:\n",
      "\n",
      "\"The Amazon Rainforest is the largest tropical rainforest in the world, spanning over 5.5 million square kilometers. It is home to an incredible diversity of wildlife, including thousands of plant species, birds, mammals, and insects. The rainforest plays a crucial role in regulating the Earth's climate and is often referred to as the 'lungs of the planet' due to its vast capacity for carbon dioxide absorption.\"\n",
      "\n",
      "Counting the tokens, we find:\n",
      "\n",
      "1. Words\n",
      "2. Punctuation marks (like commas, periods, and quotation marks)\n",
      "\n",
      "After counting, the total number of tokens in the paragraph is **102**.\n"
     ]
    }
   ],
   "source": [
    "token_prompt = \"How many tokens is in the following paragraph: \" + am_response\n",
    "\n",
    "tokens = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": token_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "token_response = tokens.choices[0].message.content\n",
    "print(token_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch...\n",
    "\n",
    "102 instead of 80.\n",
    "\n",
    "But I appreciate the effort! We can see how hard GPT-4o Mini tried to give the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs counting characters and words in simpler examples.\n",
    "\n",
    "I'll show you something interesting...\n",
    "\n",
    "I'll ask GPT-4o Mini to count words and characters in 2 sentences:\n",
    "1. \"This is a short sentence.\"\n",
    "2. \"This is a very short sentence.\"\n",
    "\n",
    "Let's see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"This is a short sentence\" contains 30 characters (including spaces) and 6 words.\n"
     ]
    }
   ],
   "source": [
    "short_sentence = \"This is a short sentence\"\n",
    "counting_prompt = \"Count characters and words in the following sentence: \" + short_sentence\n",
    "\n",
    "char_cnt1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": counting_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "char_response1 = char_cnt1.choices[0].message.content\n",
    "print(char_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"This is a very short sentence\" contains 30 characters (including spaces) and 7 words.\n"
     ]
    }
   ],
   "source": [
    "very_short_sentence = \"This is a very short sentence\"\n",
    "counting_prompt = \"Count characters and words in the following sentence: \" + very_short_sentence\n",
    "\n",
    "char_cnt2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": counting_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "char_response2 = char_cnt2.choices[0].message.content\n",
    "print(char_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, I've added an entire word, \"very,\" to the second example. But for GPT-4o Mini, both sentences have the same number of characters.\n",
    "\n",
    "Weird...\n",
    "\n",
    "Let's see the responses next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"This is a short sentence\" contains 30 characters (including spaces) and 6 words.\n",
      "The sentence \"This is a very short sentence\" contains 30 characters (including spaces) and 7 words.\n"
     ]
    }
   ],
   "source": [
    "print(char_response1)\n",
    "print(char_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting with GPT-4o.\n",
    "\n",
    "Let's do some more tests. This time we'll use GPT-4o, which is the more powerful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let's break it down:\n",
      "\n",
      "**Sentence:** \"This is a very short sentence\"\n",
      "\n",
      "**Character Count:**\n",
      "- Total characters (including spaces): 27\n",
      "- Total characters (excluding spaces): 22\n",
      "\n",
      "**Word Count:**\n",
      "- Total words: 6\n",
      "\n",
      "So, the sentence \"This is a very short sentence\" has 27 characters (including spaces), 22 characters (excluding spaces), and 6 words.\n"
     ]
    }
   ],
   "source": [
    "very_short_sentence = \"This is a very short sentence\"\n",
    "counting_prompt = \"Count characters and words in the following sentence: \" + very_short_sentence\n",
    "\n",
    "char_cnt3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": counting_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "char_response3 = char_cnt3.choices[0].message.content\n",
    "print(char_response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let's break it down:\n",
      "\n",
      "**Sentence:** \"This is a short sentence\"\n",
      "\n",
      "1. **Character Count:**\n",
      "   - Total characters including spaces: 24\n",
      "   - Total characters excluding spaces: 21\n",
      "\n",
      "2. **Word Count:**\n",
      "   - Total words: 5\n",
      "\n",
      "So, the sentence \"This is a short sentence\" has 24 characters including spaces, 21 characters excluding spaces, and 5 words.\n"
     ]
    }
   ],
   "source": [
    "counting_prompt = \"Count characters and words in the following sentence: \" + short_sentence\n",
    "\n",
    "char_cnt4 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": counting_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "char_response4 = char_cnt4.choices[0].message.content\n",
    "print(char_response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This is a very short sentence' contains 29 characters and 6 words.\n"
     ]
    }
   ],
   "source": [
    "total_characters2 = len(very_short_sentence)\n",
    "total_words2 = len(very_short_sentence.split())\n",
    "total_characters_no_spaces2 = len(very_short_sentence.replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "print(f\"'{very_short_sentence}' contains {total_characters2} characters and {total_words2} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why counting tokens?\n",
    "\n",
    "When creating AI applications, it's crucial to manage (and count) tokens for several reasons:\n",
    "1. **Cost management** - Tokens directly influence the cost of API usage.\n",
    "2. **Billing accuracy** - Token counting enables accurate usage-based billing for customers.\n",
    "3. **Performance optimization** - The number of tokens affects model performance. Monitoring token usage helps optimize prompts.\n",
    "4. **Customer transparency** - Providing real-time token usage data to customers through dashboards helps them control their spending and avoid unexpected costs.\n",
    "5. **Product optimization** - Analyzing token usage patterns can provide insights into how customers are using the AI product, informing future improvements and feature development.\n",
    "6. **Compliance and security**-  Monitoring token usage can help detect unusual patterns that might indicate security issues.\n",
    "7. **Profitability analysis** - By attributing token usage to specific customers or features, companies endure profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens for non-english languages.\n",
    "\n",
    "I want to show you how the tokens work for other languages. I'll play with Polish and German translations, because I speak both languages.\n",
    "\n",
    "### Polish Translation\n",
    "\n",
    "Let's start by asking GPT-4o Mini to translate the Amazon Rainforest description into Polish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazonia jest największym tropikalnym lasem deszczowym na świecie, zajmującym ponad 5,5 miliona kilometrów kwadratowych. Jest domem dla niesamowitej różnorodności dzikiej przyrody, w tym tysięcy gatunków roślin, ptaków, ssaków i owadów. Las deszczowy odgrywa kluczową rolę w regulacji klimatu Ziemi i często nazywany jest \"płucami planety\" ze względu na swoją ogromną zdolność do absorpcji dwutlenku węgla.\n"
     ]
    }
   ],
   "source": [
    "pl_prompt = \"Translate to Polish the following: \" + am_response\n",
    "\n",
    "polish = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": pl_prompt}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "pl_translation = polish.choices[0].message.content\n",
    "print(pl_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we've got the polish translation. Let's count words and characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 55 words and 406 characters.\n"
     ]
    }
   ],
   "source": [
    "count_words_and_chars(pl_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here are the numbers for the original description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 66 words and 413 characters.\n"
     ]
    }
   ],
   "source": [
    "count_words_and_chars(am_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "So the polish translation is slightly shorter in words and characters.\n",
    "\n",
    "What about tokens?\n",
    "\n",
    "Let me show you the tokens from tiktokenizer again.\n",
    "\n",
    "<img src=\"./images/AmazonDescPL.png\" alt=\"Amazon Description tokens\" width=\"500px\" />\n",
    "\n",
    "The Polish translation has 132 tokens, the English has 80 tokens.\n",
    "\n",
    "What a huge difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing token \"efficiency\"\n",
    "\n",
    "At the beginning I said: \"As a rule of thumb, a token corresponds to 3/4 of the word. So 100 tokens is roughly 75 words.\"\n",
    "\n",
    "Now, let's do some math to test the statement.\n",
    "\n",
    "- For English: 66 (words) / 80 (tokens) = 0.825 -> 82 words per 100 tokens.\n",
    "- For Polish: 55 (words) / 132 (tokens) ≈ 0.42 -> 42 words per 100 tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting 'r' in 'strawberry'\n",
    "\n",
    "Let's use GPT-4o and GPT-4o Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'strawberry' contains 2 R's.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "strawberry_prompt = \"How many R's are there in the word 'strawberry?'\"\n",
    "\n",
    "strawberry = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": strawberry_prompt}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "strawberry_resp = strawberry.choices[0].message.content\n",
    "print(strawberry_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "tokens = enc.encode(strawberry_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these tokens is all that GPT models receive.\n",
    "\n",
    "Later, the tokens will be converted into embeddings (long vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([19772], [101830])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens)\n",
    "\n",
    "token_berry = enc.encode(\"berry\")\n",
    "token_strawberry = enc.encode(\" strawberry\")\n",
    "token_berry, token_strawberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m strawberry_mini \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: strawberry_prompt}\n\u001b[1;32m      5\u001b[0m     ],\n\u001b[1;32m      6\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m strawberry_resp_mini \u001b[38;5;241m=\u001b[39m strawberry_mini\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(strawberry_resp_mini)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "strawberry_mini = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": strawberry_prompt}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "strawberry_resp_mini = strawberry_mini.choices[0].message.content\n",
    "print(strawberry_resp_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: ChatGPT sometimes gets it right.\n",
    "\n",
    "Look:\n",
    "\n",
    "<img src=\"./images/ChatGPTRnW.png\" alt=\"ChatGPTScreen\" width=\"500\" />\n",
    "\n",
    "But it's not because it learned how to count letters. \n",
    "\n",
    "It's because it's \"memorizing\" the correct answer for this specific question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "LLMs receive a list of tokens.\n",
    "\n",
    "But token numbers are just IDs. There's no meaning in IDs.\n",
    "\n",
    "To get the meaning, we need to turn tokens into vector embeddings.\n",
    "\n",
    "But it happens inside of the Large Language Models.\n",
    "\n",
    "I will not describe it in detail, but here's the crucial information:\n",
    "- LLMs have a so-called lookup table to match token IDs with token embeddings\n",
    "\n",
    "TODO:\n",
    "- Add More crucial parts\n",
    "\n",
    "Now, we don't have access to any parameters inside of GPT-4 models, so I can't show you the token embeddings for them.\n",
    "\n",
    "Luckily, we can look inside of open-source models. So let me use the BERT model, to show you the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2026, 2171, 2003, 19031, 1012]\n",
      "Raw tokens: ['my', 'name', 'is', 'kris', '.']\n",
      "Embeddings:  tensor([[[-0.0736, -0.3184, -0.0209,  ..., -0.1902,  0.6918,  0.4574],\n",
      "         [-0.6156, -0.2890,  0.1733,  ..., -0.2697,  0.7416,  0.2899],\n",
      "         [-1.0302, -0.0216,  0.3159,  ..., -0.1907,  0.7589,  0.9509],\n",
      "         [-0.2899, -0.7108,  0.3310,  ..., -0.2164,  0.8269,  0.2375],\n",
      "         [-0.2973, -0.9342,  0.3401,  ...,  0.1431,  0.9154,  0.1736]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text to be tokenized\n",
    "text = \"My name is Kris.\"\n",
    "\n",
    "# Encode text\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "# Output the token IDs\n",
    "print(\"Token IDs:\", input_ids)\n",
    "\n",
    "# Convert token IDs back to raw tokens and output them\n",
    "raw_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "print(\"Raw tokens:\", raw_tokens)\n",
    "\n",
    "# Convert list of IDs to a tensor\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Pass the input through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor)\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Print the embeddings\n",
    "print(\"Embeddings: \", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline; \n",
    "print(pipeline('sentiment-analysis')('we love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9997424483299255}]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline('sentiment-analysis')(\"What a boring movie!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Mountains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount Everest, the highest peak in the world, stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. Located in the Himalayas on the border between Nepal and Tibet, it attracts thousands of climbers each year, despite the extreme conditions and risks associated with its ascent. The mountain is also significant culturally, revered in local traditions and considered sacred by the Sherpa people.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write 3 sentences about Mount Everest\"}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the response:\n",
    "\n",
    "```\n",
    "Mount Everest, the highest peak in the world, stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. Located in the Himalayas on the border between Nepal and Tibet, it attracts thousands of climbers each year, including seasoned mountaineers and ambitious adventurers. Despite its allure, climbing Everest presents significant challenges, including extreme weather, high altitudes, and the risk of avalanches.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions.\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- [Embedding tokens vs embedding string](https://community.openai.com/t/embedding-tokens-vs-embedding-strings/463213/5)\n",
    "- [BERT token vs embedding](https://stackoverflow.com/questions/77189885/bert-token-vs-embedding)\n",
    "- [Explained: Tokens and Embeddings in LLMs](https://medium.com/the-research-nest/explained-tokens-and-embeddings-in-llms-69a16ba5db33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
