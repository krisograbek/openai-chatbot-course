{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "\n",
    "In this notebook, you'll learn about tokens - a crucial concept tied directly to Large Language Models (LLMs).\n",
    "\n",
    "\n",
    "\n",
    "Here's what you will learn:\n",
    "- What is a token?\n",
    "- Why do we use tokens?\n",
    "- Why LLMs are so bad at counting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n",
    "\n",
    "A token is a chunk of text that Large Language Models read or generate.\n",
    "\n",
    "Here's key information about tokens:\n",
    "- A token is the smallest unit of text that AI models process.\n",
    "- Tokens don't have the defined length. Some are only 1 character long, others can be longer words.\n",
    "- Tokens can be: words, sub-words, punctuation marks or special symbols.\n",
    "- As a rule of thumb, a token corresponds to 3/4 of the word. So 100 tokens is roughly 75 words.\n",
    "\n",
    "So let me show you how to count tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why tokens (not characters or words)?\n",
    "\n",
    "#TODO: \n",
    "- Try to explain the drawbacks of character and word approach.\n",
    "- Perplexity: Why do we use tokens in LLMs, not characters or words?\n",
    "- Explain that tokens are the combination of the best from both worlds.\n",
    "- Explain tokens are turned into embeddings inside of LLMs, as they have a lookup table in which they get IDs (tokens). The embedding values are trainable and get updated during the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting 'r' in 'strawberry'\n",
    "\n",
    "Let's use GPT-4o and GPT-4o Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two R's in the word \"strawberry.\"\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "strawberry_prompt = \"How many R's are there in the word 'strawberry?'\"\n",
    "\n",
    "strawberry = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": strawberry_prompt}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "strawberry_resp = strawberry.choices[0].message.content\n",
    "print(strawberry_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'strawberry' contains 2 'R's.\n"
     ]
    }
   ],
   "source": [
    "strawberry_mini = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": strawberry_prompt}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "strawberry_resp_mini = strawberry_mini.choices[0].message.content\n",
    "print(strawberry_resp_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you, what GPT models see.\n",
    "\n",
    "We'll use the `tiktoken` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "tokens = enc.encode(strawberry_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5299, 1991, 460, 885, 553, 1354, 306, 290, 2195, 461, 302, 1618, 19772, 127222]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these tokens is all that GPT models receive.\n",
    "\n",
    "Later, the tokens will be converted into embeddings (long vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([19772], [101830])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_berry = enc.encode(\"berry\")\n",
    "token_strawberry = enc.encode(\" strawberry\")\n",
    "token_berry, token_strawberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: ChatGPT sometimes gets it right.\n",
    "\n",
    "Look:\n",
    "\n",
    "<img src=\"./images/ChatGPTRnW.png\" alt=\"ChatGPTScreen\" width=\"500\" />\n",
    "\n",
    "But it's not because it learned how to count letters. \n",
    "\n",
    "It's because it's \"memorizing\" the correct answer for this specific question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words, letters and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amazon Rainforest, often referred to as the \"lungs of the Earth,\" is the largest tropical rainforest in the world, spanning across several countries in South America, including Brazil, Peru, and Colombia. It is home to an incredible diversity of flora and fauna, with millions of species, many of which are not found anywhere else on the planet. However, the rainforest faces significant threats from deforestation, climate change, and industrial activities, which jeopardize its ecological balance and the livelihoods of indigenous communities.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "am_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write 3 sentences about Amazon Rainforest\"}\n",
    "    ],\n",
    "    seed=42,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "am_response = am_completion.choices[0].message.content\n",
    "print(am_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy:\n",
    "\n",
    "\n",
    "```\n",
    "The Amazon Rainforest, often referred to as the \"lungs of the Earth,\" is the largest tropical rainforest in the world, spanning across several countries in South America, including Brazil, Peru, and Colombia. It is home to an incredible diversity of flora and fauna, with millions of species, many of which are not found anywhere else on the planet. However, the rainforest faces significant threats from deforestation, climate change, and industrial activities, which jeopardize its ecological balance and the livelihoods of indigenous communities.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've got a short description about my country, Poland.\n",
    "\n",
    "Let's count words and characters first. In Python it's quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 82 words and 549 characters.\n"
     ]
    }
   ],
   "source": [
    "words_cnt = len(am_response.split())\n",
    "characters_cnt = len(am_response)\n",
    "\n",
    "print(f\"The response has {words_cnt} words and {characters_cnt} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask GPT-4o Mini the same questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph contains 81 words.\n"
     ]
    }
   ],
   "source": [
    "word_prompt = \"How many words is in the following paragraph: \" + am_response\n",
    "\n",
    "words = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": word_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "word_response = words.choices[0].message.content\n",
    "print(word_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph you provided contains 570 characters, including spaces and punctuation.\n"
     ]
    }
   ],
   "source": [
    "character_prompt = \"How many characters is in the following paragraph: \" + am_response\n",
    "\n",
    "characters = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": character_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "character_response = characters.choices[0].message.content\n",
    "print(character_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of tokens in the provided paragraph, we can break it down into individual words and punctuation marks. In natural language processing, a token typically refers to a word or a punctuation mark.\n",
      "\n",
      "The paragraph you provided contains 86 tokens.\n"
     ]
    }
   ],
   "source": [
    "token_prompt = \"How many tokens is in the following paragraph: \" + am_response\n",
    "\n",
    "tokens = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": token_prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "token_response = tokens.choices[0].message.content\n",
    "print(token_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting tokens.\n",
    "\n",
    "To count tokens, we'll use the `tiktoken` library.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "tokens = enc.encode(am_response)\n",
    "print(f\"The response has {len(tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break the code down:\n",
    "- We imported the tiktoken library.\n",
    "- We defined the encoder using `encoding_for_model(\"gpt-4o-mini\")` to ensure we use the right encoder.\n",
    "- We \"tokenized\" the response using `encode(pl_response)`.\n",
    "- We counted the tokens using Python's `len` function.\n",
    "\n",
    "Great!\n",
    "\n",
    "Let's take our sample text and run it through the [online tokenizer](https://tiktokenizer.vercel.app/).\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "<img src=\"./images/AmazonDescTokens.png\" alt=\"Amazon Description tokens\" width=\"500px\" />\n",
    "\n",
    "I love that visual representation. The app highlights every single token. It helps us see how they actually look like.\n",
    "\n",
    "Below, we can see the numerical representation of each token from the decription.\n",
    "\n",
    "Let's try to see, if the numbers match with the tokens from the `tiktoken` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why counting tokens?\n",
    "\n",
    "When creating AI applications, it's crucial to manage (and count) tokens for several reasons:\n",
    "1. **Cost management** - Tokens directly influence the cost of API usage.\n",
    "2. **Billing accuracy** - Token counting enables accurate usage-based billing for customers.\n",
    "3. **Performance optimization** - The number of tokens affects model performance. Monitoring token usage helps optimize prompts.\n",
    "4. **Customer transparency** - Providing real-time token usage data to customers through dashboards helps them control their spending and avoid unexpected costs.\n",
    "5. **Product optimization** - Analyzing token usage patterns can provide insights into how customers are using the AI product, informing future improvements and feature development.\n",
    "6. **Compliance and security**-  Monitoring token usage can help detect unusual patterns that might indicate security issues.\n",
    "7. **Profitability analysis** - By attributing token usage to specific customers or features, companies endure profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polish Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Translate to Polish the following: \" + am_response\n",
    "\n",
    "polish = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "translation = polish.choices[0].message.content\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Mountains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount Everest, the highest peak in the world, stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. Located in the Himalayas on the border between Nepal and Tibet, it attracts thousands of climbers each year, despite the extreme conditions and risks associated with its ascent. The mountain is also significant culturally, revered in local traditions and considered sacred by the Sherpa people.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write 3 sentences about Mount Everest\"}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the response:\n",
    "\n",
    "```\n",
    "Mount Everest, the highest peak in the world, stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. Located in the Himalayas on the border between Nepal and Tibet, it attracts thousands of climbers each year, including seasoned mountaineers and ambitious adventurers. Despite its allure, climbing Everest presents significant challenges, including extreme weather, high altitudes, and the risk of avalanches.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
