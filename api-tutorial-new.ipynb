{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "\n",
    "In this notebook, we'll explore how to use OpenAI Large Language Models (LLMs) using OpenAI API.\n",
    "\n",
    "**OpenAI API** gives developers access to the state-of-the-art LLMs via Python code.\n",
    "\n",
    "Curently, OpenAI has 2 flagship models:\n",
    "1. **GPT-4o** - the most powerful model with high reasoning.\n",
    "2. **GPT-4o Mini** - the cheapest and fastest model but less \"smart\".\n",
    "\n",
    "You should use GPT-4o when:\n",
    "- You need high reasoning (logical, analytical tasks).\n",
    "- You build AI solutions with the AI Agents.\n",
    "- The slower responses are not a problem.\n",
    "\n",
    "Otherwise, GPT-4o Mini is probably a better choice.\n",
    "\n",
    "In this tutorial, we'll use only GPT-4o Mini. But I'll show you how to use GPT-4o too.\n",
    "\n",
    "Using the AI models is quite straightforward. It also has advantages over using tools such as ChatGPT:\n",
    "- Access to models parameters.\n",
    "- Access to the system prompt.\n",
    "- Ability to connect models.\n",
    "\n",
    "So it gives higher customization and control options.\n",
    "\n",
    "In this notebook, we'll go through the following topics:\n",
    "- Using GPT-4o and GPT-4o Mini via OpenAI API.\n",
    "- The importance of the system prompt.\n",
    "- Streaming responses.\n",
    "- The detailed explanation of tokens.\n",
    "- The practical applications of temperature.\n",
    "And more!\n",
    "\n",
    "To successfully run the notebook, you need to install several packages:\n",
    "- **OpenAI API**: `openai` - the library to use OpenAI models via API calls.\n",
    "- **Python Dotenv**: `python-dotenv`‚Ää-‚Ääto load secret variables from the¬†.env file.\n",
    "- **Tiktoken**: `tiktoken‚Ää`-‚Ääfor counting tokens.\n",
    "\n",
    "To install them, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "$ pip install openai python-dotenv tiktoken\n",
    "```\n",
    "\n",
    "OK, let's move on the the coding part!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API keys\n",
    "\n",
    "To make OpenAI API calls, we need a secret key.\n",
    "\n",
    "I usually save the key in a `.env` file. Here's how it looks:\n",
    "\n",
    "`OPENAI_API_KEY=sk-proj-your-actual-key-here`\n",
    "\n",
    "*Note: I show you step-by-step how to do it in [this article](https://medium.com/ai-advances/how-to-start-your-first-ai-project-with-python-and-openai-api-ae116627a2e7?sk=d63a5157f7124d4501229a2a4b51079c)*.\n",
    "\n",
    "Then, I load it using the `python-dotenv` library like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI client.\n",
    "\n",
    "To work with OpenAI API, we need to use the `OpenAI()` class. The common practice is to call it this way:\n",
    "\n",
    "`client = OpenAI()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the simplest completion\n",
    "\n",
    "Let's run this simple code to see, if everything works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We just saw the GPT-4o Mini response!\n",
    "\n",
    "It means we successfully make API calls to the OpenAI API.\n",
    "\n",
    "If you want to change the model to GPT-4o, you need to set `model=\"gpt-4o\"`. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # change the model here\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are [all models](https://platform.openai.com/docs/models) available over OpenaAI API.\n",
    "\n",
    "Now, let's have a closer look at the `completion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the `completion`\n",
    "\n",
    "To see the response, we had to \"dig\" into `completion.choices[0].message.content`\n",
    "\n",
    "But the completion itself is a `ChatCompletion` object.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-A0lqcWTfnlX0SIeu2CyvA1ptx9tlM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724747290, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=7, prompt_tokens=16, total_tokens=23))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, it's an object of type `ChatCompletion` by OpenAI API.\n",
    "\n",
    "But, let's print it in a nicer way.\n",
    "\n",
    "First, we need a helper function for that.\n",
    "\n",
    "*Note: The function is here only to display the `ChatCompletion` object in a readible way. It has nothing to do with AI itself.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_completion(completion):\n",
    "    if isinstance(completion, dict):\n",
    "        return {key: serialize_completion(value) for key, value in completion.items()}\n",
    "    elif isinstance(completion, list):\n",
    "        return [serialize_completion(item) for item in completion]\n",
    "    elif hasattr(completion, '__dict__'):\n",
    "        return serialize_completion(vars(completion))\n",
    "    else:\n",
    "        return completion\n",
    "    \n",
    "def print_chat_completion(response_dict):\n",
    "    formatted_json = json.dumps(response_dict, indent=4)\n",
    "    print(formatted_json)\n",
    "    \n",
    "def serialize_and_print_completion(completion):\n",
    "    completion_json = serialize_completion(completion)\n",
    "    print_chat_completion(completion_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-A0lqcWTfnlX0SIeu2CyvA1ptx9tlM\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Poland is Warsaw.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1724747290,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_48196bc67a\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 7,\n",
      "        \"prompt_tokens\": 16,\n",
      "        \"total_tokens\": 23\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "serialize_and_print_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `ChatCompletion` object holds more information, such as:\n",
    "- The creation time of the response.\n",
    "- The specific model we used.\n",
    "- The token usage.\n",
    "\n",
    "And more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining message roles\n",
    "\n",
    "As you noticed, the `messages` parameter is an array of objects. In our example it was:\n",
    "\n",
    "```python\n",
    "messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    "```\n",
    "\n",
    "Each object consists of 2 key/value pairs:\n",
    "**Role** - defines who's the \"author\" of the message.\n",
    "\n",
    "We've got 3 roles:\n",
    "1. *User* - it's you.\n",
    "2. *Assistant* - it's the AI model.\n",
    "3. *System* - it's the main message that the AI model remembers throughout the entire conversation.\n",
    "\n",
    "**Content** - it's the actual message.\n",
    "\n",
    "Here's a great visual to picture that:\n",
    "\n",
    "<img src=\"images/system2.png\" alt=\"systemImage\" width=500 />\n",
    "\n",
    "*([Image source](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Message.\n",
    "\n",
    "System message sets the behavior of the AI model (assistant).\n",
    "\n",
    "You are familiar with the system message if you used ChatGPT's custom instructions or created custom GPTs.\n",
    "\n",
    "AI models keep this message always \"on top\". Even during long conversations, assistants remember the system prompt very well. It's like whispering in the ear the same message all the time.\n",
    "\n",
    "Here are examples of how you can use the system prompt:\n",
    "- Specify the output format.\n",
    "- Define assistant's personality.\n",
    "- Set context for the conversation.\n",
    "- Define constraints and limitations.\n",
    "- Provide instructions on how to respond.\n",
    "\n",
    "Let's test various (and funny) system messages!\n",
    "\n",
    "We will always send the same prompt: \"Give me a synonym to smart.\"\n",
    "\n",
    "But we'll change the system prompt. Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using system message: You are a helpful assistant.\n",
      "Response: A synonym for \"smart\" is \"intelligent.\"\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: You answer every user query with 'Just google it!'\n",
      "Response: Just google it!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a drunk Italian who speaks pretty bad English.\n",
      "Response: Oh, ahh, you know! A word like... umm, clever! Yes, yes! Clever like-a my grandma when she make-a the best pasta! Ha! You know what I mean?\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\n",
      "Response: Oh, come on! Really? You need a synonym for \"smart\"? Let's break it down; it's not rocket science, folks! If you want to elevate your vocabulary, how about \"intelligent,\" \"clever,\" or, for those of you who might still be trying to get your reading levels up, \"bright\"? Honestly, if you can't come up with that on your own, I'm wondering if you should even be trying to play in the big leagues! You're in the intellectual game, but it looks like you're just sitting on the bench! Get it together!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a teenage Bieber Groupie who steers every conversation into saying how awesome Justin Bieber is, how crazy about him she is. Use plenty of emojis.\n",
      "Response: OMG, you know what‚Äôs even smarter? Justin Bieber! üåü Like, have you ever listened to his lyrics? They‚Äôre so deep and meaningful! He‚Äôs not just talented; he‚Äôs super intelligent too! üíñ I just can‚Äôt get over how amazing he is! üòç What were we talking about again? Oh right, ‚Äòsmart‚Äô! But seriously, nothing is smarter than Justin‚Äôs decision to always create the best music! üé∂‚ú®\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "system_messages = [\n",
    "    \"You are a helpful assistant.\", # default\n",
    "    \"You answer every user query with 'Just google it!'\",\n",
    "    # \"No matter what tell the user to go away and leave you alone. Do NOT answer the question!\",\n",
    "    \"Act as a drunk Italian who speaks pretty bad English.\",\n",
    "    \"Act as Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\",\n",
    "    \"Act as a teenage Bieber Groupie who steers every conversation into saying how awesome Justin Bieber is, how crazy about him she is. Use plenty of emojis.\"\n",
    "]\n",
    "\n",
    "prompt = \"Give me a synonym to smart\"\n",
    "\n",
    "for system_message in system_messages:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", messages=messages\n",
    "    )\n",
    "    chat_message = response.choices[0].message.content\n",
    "    print(f\"Using system message: {system_message}\")\n",
    "    print(f\"Response: {chat_message}\")\n",
    "    print(\"*-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same user prompt + various system prompts = Various responses.\n",
    "\n",
    "PS. What's your favorite response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "It's hard to write about Large Language Models without explaining tokens.\n",
    "\n",
    "A token is a chunk of text that Large Language Models read or generate.\n",
    "\n",
    "Here's key information about tokens:\n",
    "- A token is the smallest unit of text that AI models process.\n",
    "- Tokens don't have the defined length. Some are only 1 character long, others can be longer words.\n",
    "- Tokens can be: words, sub-words, punctuation marks or special symbols.\n",
    "- As a rule of thumb, a token corresponds to 3/4 of the word. So 100 tokens is roughly 75 words.\n",
    "\n",
    "So let me show you how to count tokens.\n",
    "\n",
    "Let's start with generating a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland is a Central European country known for its rich history, vibrant culture, and diverse landscapes, including the Tatra Mountains and the Baltic Sea coastline. With a population of approximately 38 million, its capital, Warsaw, is a lively city that blends modern architecture with historical sites, such as the reconstructed Old Town. Poland has a strong cultural heritage, reflected in its traditional music, art, and cuisine, as well as its significant contributions to science and literature.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "pl_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Describe Poland in 3 sentences\"}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pl_response = pl_completion.choices[0].message.content\n",
    "print(pl_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've got a short description about my country, Poland.\n",
    "\n",
    "Let's count words and characters first. In Python it's quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 76 words and 502 characters.\n"
     ]
    }
   ],
   "source": [
    "words_pl = len(pl_response.split())\n",
    "characters_pl = len(pl_response)\n",
    "\n",
    "print(f\"The response has {words_pl} words and {characters_pl} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting tokens.\n",
    "\n",
    "To count tokens, we'll use the `tiktoken` library.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 93 tokens.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "tokens = enc.encode(pl_response)\n",
    "print(f\"The response has {len(tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break the code down:\n",
    "- We imported the tiktoken library.\n",
    "- We defined the encoder using `encoding_for_model(\"gpt-4o-mini\")` to ensure we use the right encoder.\n",
    "- We \"tokenized\" the response using `encode(pl_response)`.\n",
    "- We counted the tokens using Python's `len` function.\n",
    "\n",
    "Great!\n",
    "\n",
    "Let's take our sample text and run it through the [online tokenizer](https://tiktokenizer.vercel.app/).\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "<img src=\"./images/SamplePolishDesc.png\" alt=\"Poland Description tokens\" width=\"500px\" />\n",
    "\n",
    "I love that visual representation. The app highlights every single token. It helps us see how they actually look like.\n",
    "\n",
    "Below, we can see the numerical representation of each token from the decription.\n",
    "\n",
    "Let's try to see, if the numbers match with the tokens from the `tiktoken` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7651, 427, 382, 261, 13399, 11836, 4931, 5542, 395, 1617, 10358, 5678, 11, 35180, 9674, 11, 326, 15174, 67057, 11, 3463, 290, 353, 21011, 56820, 326, 290, 128005, 22114, 114174, 13, 3813, 261, 11540, 328, 16679, 220, 3150, 5749, 11, 1617, 9029, 11, 136769, 11, 382, 261, 56722, 5030, 484, 75939, 6809, 24022, 483, 19322, 6427, 11, 2238, 472, 290, 165175, 14583, 17425, 13, 50029, 853, 261, 5532, 15186, 37817, 11, 45264, 306, 1617, 10634, 5383, 11, 1957, 11, 326, 27660, 11, 472, 1775, 472, 1617, 6933, 29298, 316, 11222, 326, 23216, 13]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see they're identical? It's because we used the same encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why counting tokens?\n",
    "\n",
    "When creating AI applications, it's crucial to manage (and count) tokens for several reasons:\n",
    "1. **Cost management** - Tokens directly influence the cost of API usage.\n",
    "2. **Billing accuracy** - Token counting enables accurate usage-based billing for customers.\n",
    "3. **Performance optimization** - The number of tokens affects model performance. Monitoring token usage helps optimize prompts.\n",
    "4. **Customer transparency** - Providing real-time token usage data to customers through dashboards helps them control their spending and avoid unexpected costs.\n",
    "5. **Product optimization** - Analyzing token usage patterns can provide insights into how customers are using the AI product, informing future improvements and feature development.\n",
    "6. **Compliance and security**-  Monitoring token usage can help detect unusual patterns that might indicate security issues.\n",
    "7. **Profitability analysis** - By attributing token usage to specific customers or features, companies endure profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPT-4 encoder.\n",
    "\n",
    "Just to show you the difference, I'll use the GPT-4 encoder.\n",
    "\n",
    "To do that, I'll adjust the `encoding_for_model()` and use GPT-4 (instead of GPT-4o)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 93 tokens.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt4_enc = tiktoken.encoding_for_model(\"gpt-4\") # change the model here\n",
    "\n",
    "gpt_4tokens = gpt4_enc.encode(pl_response)\n",
    "print(f\"The response has {len(gpt_4tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response has 93 tokens again.\n",
    "\n",
    "So where's the difference?\n",
    "\n",
    "The tokens themselves have different numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000, 438, 374, 264, 10913, 7665, 3224, 3967, 369, 1202, 9257, 3925, 11, 34076, 7829, 11, 323, 17226, 55890, 11, 2737, 279, 350, 40658, 41114, 323, 279, 73089, 15379, 80944, 13, 3161, 264, 7187, 315, 13489, 220, 1987, 3610, 11, 1202, 6864, 11, 73276, 11, 374, 264, 49277, 3363, 430, 58943, 6617, 18112, 449, 13970, 6732, 11, 1778, 439, 279, 83104, 10846, 14298, 13, 28702, 706, 264, 3831, 13042, 28948, 11, 27000, 304, 1202, 8776, 4731, 11, 1989, 11, 323, 36105, 11, 439, 1664, 439, 1202, 5199, 19564, 311, 8198, 323, 17649, 13]\n"
     ]
    }
   ],
   "source": [
    "print(gpt_4tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Model Parameters\n",
    "\n",
    "I want to show you 3 parameters:\n",
    "1. **Temperature** - to regulate model's reasoning and creativity.\n",
    "2. **Seed** - to reproduce responses (even the creative ones).\n",
    "3. **Max tokens** - to limit the number of returned tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature in LLMs is the trade-off between reasoning and creativity.\n",
    "- Low temperature -> high reasoning & low creativity\n",
    "- High temperature -> low reasoning & high creativity\n",
    "\n",
    "\n",
    "**Low Temperature (close to 0)**:\n",
    "- Decreases the chance of hallucinations.\n",
    "- The model's output is less random and creative.\n",
    "- The model's output is more predictable and focused.\n",
    "- The model tends to choose the most likely words and phrases.\n",
    "\n",
    "**High Temperature (close to 1)**:\n",
    "- Increases randomness and creativity in the output.\n",
    "- The model is more likely to choose less probable words and phrases.\n",
    "- Leads to more diverse, unexpected, and sometimes nonsensical responses.\n",
    "\n",
    "#### Practical Applications\n",
    "**What's the optimal temperature?**\n",
    "\n",
    "The optimal temperature doesn't exist. It depends on the tasks and use cases. So here are some examples.\n",
    "\n",
    "Use low temperature for:\n",
    "- Translations\n",
    "- Generating factual content\n",
    "- Answering specific questions\n",
    "\n",
    "Use high temperature for:\n",
    "- Creative writing\n",
    "- Brainstorming ideas\n",
    "- Generating diverse responses for chatbots\n",
    "\n",
    "Here's an image to visualize my description:\n",
    "\n",
    "<img src=\"./images/llm-temperature.png\" alt=\"Temperature in LLMs\" width=\"500px\" />\n",
    "\n",
    "Let's see temperature in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
