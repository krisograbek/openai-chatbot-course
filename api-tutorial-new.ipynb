{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "\n",
    "In this notebook, we'll explore how to use OpenAI Large Language Models (LLMs) using OpenAI API.\n",
    "\n",
    "**OpenAI API** gives developers access to the state-of-the-art LLMs via Python code.\n",
    "\n",
    "Curently, OpenAI has 2 flagship models:\n",
    "1. **GPT-4o** - the most powerful model with high reasoning.\n",
    "2. **GPT-4o Mini** - the cheapest and fastest model but less \"smart\".\n",
    "\n",
    "You should use GPT-4o when:\n",
    "- You need high reasoning (logical, analytical tasks).\n",
    "- You build AI solutions with the AI Agents.\n",
    "- The slower responses are not a problem.\n",
    "\n",
    "Otherwise, GPT-4o Mini is probably a better choice.\n",
    "\n",
    "In this tutorial, we'll use only GPT-4o Mini. But I'll show you how to use GPT-4o too.\n",
    "\n",
    "Using the AI models is quite straightforward. It also has advantages over using tools such as ChatGPT:\n",
    "- Access to models parameters.\n",
    "- Access to the system prompt.\n",
    "- Ability to connect models.\n",
    "\n",
    "So it gives higher customization and control options.\n",
    "\n",
    "In this notebook, we'll go through the following topics:\n",
    "- Using GPT-4o and GPT-4o Mini via OpenAI API.\n",
    "- The importance of the system prompt.\n",
    "- Streaming responses.\n",
    "- The detailed explanation of tokens.\n",
    "- The practical applications of temperature.\n",
    "And more!\n",
    "\n",
    "To successfully run the notebook, you need to install several packages:\n",
    "- **OpenAI API**: `openai` - the library to use OpenAI models via API calls.\n",
    "- **Python Dotenv**: `python-dotenv` - to load secret variables from the .env file.\n",
    "- **Tiktoken**: `tiktoken `- for counting tokens.\n",
    "\n",
    "To install them, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "$ pip install openai python-dotenv tiktoken\n",
    "```\n",
    "\n",
    "OK, let's move on the the coding part!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API keys\n",
    "\n",
    "To make OpenAI API calls, we need a secret key.\n",
    "\n",
    "I usually save the key in a `.env` file. Here's how it looks:\n",
    "\n",
    "`OPENAI_API_KEY=sk-proj-your-actual-key-here`\n",
    "\n",
    "*Note: I show you step-by-step how to do it in [this article](https://medium.com/ai-advances/how-to-start-your-first-ai-project-with-python-and-openai-api-ae116627a2e7?sk=d63a5157f7124d4501229a2a4b51079c)*.\n",
    "\n",
    "Then, I load it using the `python-dotenv` library like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI client.\n",
    "\n",
    "To work with OpenAI API, we need to use the `OpenAI()` class. The common practice is to call it this way:\n",
    "\n",
    "`client = OpenAI()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the simplest completion\n",
    "\n",
    "Let's run this simple code to see, if everything works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We just saw the GPT-4o Mini response!\n",
    "\n",
    "It means we successfully make API calls to the OpenAI API.\n",
    "\n",
    "If you want to change the model to GPT-4o, you need to set `model=\"gpt-4o\"`. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # change the model here\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are [all models](https://platform.openai.com/docs/models) available over OpenaAI API.\n",
    "\n",
    "Now, let's have a closer look at the `completion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the `completion`\n",
    "\n",
    "To see the response, we had to \"dig\" into `completion.choices[0].message.content`\n",
    "\n",
    "But the completion itself is a `ChatCompletion` object.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-A0lqcWTfnlX0SIeu2CyvA1ptx9tlM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724747290, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=7, prompt_tokens=16, total_tokens=23))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, it's an object of type `ChatCompletion` by OpenAI API.\n",
    "\n",
    "But, let's print it in a nicer way.\n",
    "\n",
    "First, we need a helper function for that.\n",
    "\n",
    "*Note: The function is here only to display the `ChatCompletion` object in a readible way. It has nothing to do with AI itself.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_completion(completion):\n",
    "    if isinstance(completion, dict):\n",
    "        return {key: serialize_completion(value) for key, value in completion.items()}\n",
    "    elif isinstance(completion, list):\n",
    "        return [serialize_completion(item) for item in completion]\n",
    "    elif hasattr(completion, '__dict__'):\n",
    "        return serialize_completion(vars(completion))\n",
    "    else:\n",
    "        return completion\n",
    "    \n",
    "def print_chat_completion(response_dict):\n",
    "    formatted_json = json.dumps(response_dict, indent=4)\n",
    "    print(formatted_json)\n",
    "    \n",
    "def serialize_and_print_completion(completion):\n",
    "    completion_json = serialize_completion(completion)\n",
    "    print_chat_completion(completion_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-A0lqcWTfnlX0SIeu2CyvA1ptx9tlM\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Poland is Warsaw.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1724747290,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_48196bc67a\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 7,\n",
      "        \"prompt_tokens\": 16,\n",
      "        \"total_tokens\": 23\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "serialize_and_print_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `ChatCompletion` object holds more information, such as:\n",
    "- The creation time of the response.\n",
    "- The specific model we used.\n",
    "- The token usage.\n",
    "\n",
    "And more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining message roles\n",
    "\n",
    "As you noticed, the `messages` parameter is an array of objects. In our example it was:\n",
    "\n",
    "```python\n",
    "messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    "```\n",
    "\n",
    "Each object consists of 2 key/value pairs:\n",
    "**Role** - defines who's the \"author\" of the message.\n",
    "\n",
    "We've got 3 roles:\n",
    "1. *User* - it's you.\n",
    "2. *Assistant* - it's the AI model.\n",
    "3. *System* - it's the main message that the AI model remembers throughout the entire conversation.\n",
    "\n",
    "**Content** - it's the actual message.\n",
    "\n",
    "Here's a great visual to picture that:\n",
    "\n",
    "<img src=\"images/system2.png\" alt=\"systemImage\" width=500 />\n",
    "\n",
    "*([Image source](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Message.\n",
    "\n",
    "System message sets the behavior of the AI model (assistant).\n",
    "\n",
    "AI models keep this message always \"on top\". Even during long conversations, assistants remember the system prompt very well. It's like whispering in the ear the same message all the time.\n",
    "\n",
    "Here are examples of how you can use the system prompt:\n",
    "- Specify the output format.\n",
    "- Define assistant's personality.\n",
    "- Set context for the conversation.\n",
    "- Define constraints and limitations.\n",
    "- Provide instructions on how to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
