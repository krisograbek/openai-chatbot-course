{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "\n",
    "In this notebook, we'll explore how to use OpenAI Large Language Models (LLMs) using OpenAI API.\n",
    "\n",
    "**OpenAI API** gives developers access to the state-of-the-art LLMs via Python code.\n",
    "\n",
    "Curently, OpenAI has 2 flagship models:\n",
    "1. **GPT-4o** - the most powerful model with high reasoning.\n",
    "2. **GPT-4o Mini** - the cheapest and fastest model but less \"smart\".\n",
    "\n",
    "You should use GPT-4o when:\n",
    "- You need high reasoning (logical, analytical tasks).\n",
    "- You build AI solutions with the AI Agents.\n",
    "- The slower responses are not a problem.\n",
    "\n",
    "Otherwise, GPT-4o Mini is probably a better choice.\n",
    "\n",
    "In this tutorial, we'll use only GPT-4o Mini. But I'll show you how to use GPT-4o too.\n",
    "\n",
    "Using the AI models is quite straightforward. It also has advantages over using tools such as ChatGPT:\n",
    "- Access to models parameters.\n",
    "- Access to the system prompt.\n",
    "- Ability to connect models.\n",
    "\n",
    "So it gives higher customization and control options.\n",
    "\n",
    "In this notebook, we'll go through the following topics:\n",
    "- Using GPT-4o and GPT-4o Mini via OpenAI API.\n",
    "- The importance of the system prompt.\n",
    "- Streaming responses.\n",
    "- The detailed explanation of tokens.\n",
    "- The practical applications of temperature.\n",
    "And more!\n",
    "\n",
    "To successfully run the notebook, you need to install several packages:\n",
    "- **OpenAI API**: `openai` - the library to use OpenAI models via API calls.\n",
    "- **Python Dotenv**: `python-dotenv` - to load secret variables from the .env file.\n",
    "- **Tiktoken**: `tiktoken `- for counting tokens.\n",
    "\n",
    "To install them, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "$ pip install openai python-dotenv tiktoken\n",
    "```\n",
    "\n",
    "OK, let's move on the the coding part!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API keys\n",
    "\n",
    "To make OpenAI API calls, we need a secret key.\n",
    "\n",
    "I usually save the key in a `.env` file. Here's how it looks:\n",
    "\n",
    "`OPENAI_API_KEY=sk-proj-your-actual-key-here`\n",
    "\n",
    "*Note: I show you step-by-step how to do it in [this article](https://medium.com/ai-advances/how-to-start-your-first-ai-project-with-python-and-openai-api-ae116627a2e7?sk=d63a5157f7124d4501229a2a4b51079c)*.\n",
    "\n",
    "Then, I load it using the `python-dotenv` library like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI client.\n",
    "\n",
    "To work with OpenAI API, we need to use the `OpenAI()` class. The common practice is to call it this way:\n",
    "\n",
    "`client = OpenAI()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the simplest completion\n",
    "\n",
    "Let's run this simple code to see, if everything works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We just saw the GPT-4o Mini response!\n",
    "\n",
    "It means we successfully make API calls to the OpenAI API.\n",
    "\n",
    "If you want to change the model to GPT-4o, you need to set `model=\"gpt-4o\"`. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # change the model here\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are [all models](https://platform.openai.com/docs/models) available over OpenaAI API.\n",
    "\n",
    "Now, let's have a closer look at the `completion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the `completion`\n",
    "\n",
    "To see the response, we had to \"dig\" into `completion.choices[0].message.content`\n",
    "\n",
    "But the completion itself is a `ChatCompletion` object.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-A0pPYz9LqnH66alz9H9gRau5FE50m', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724760988, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_a2ff031fb5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, it's an object of type `ChatCompletion` by OpenAI API.\n",
    "\n",
    "But, let's print it in a nicer way.\n",
    "\n",
    "First, we need a helper function for that.\n",
    "\n",
    "*Note: The function is here only to display the `ChatCompletion` object in a readible way. It has nothing to do with AI itself.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_completion(completion):\n",
    "    if isinstance(completion, dict):\n",
    "        return {key: serialize_completion(value) for key, value in completion.items()}\n",
    "    elif isinstance(completion, list):\n",
    "        return [serialize_completion(item) for item in completion]\n",
    "    elif hasattr(completion, '__dict__'):\n",
    "        return serialize_completion(vars(completion))\n",
    "    else:\n",
    "        return completion\n",
    "    \n",
    "def print_chat_completion(response_dict):\n",
    "    formatted_json = json.dumps(response_dict, indent=4)\n",
    "    print(formatted_json)\n",
    "    \n",
    "def serialize_and_print_completion(completion):\n",
    "    completion_json = serialize_completion(completion)\n",
    "    print_chat_completion(completion_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-A0pPYz9LqnH66alz9H9gRau5FE50m\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Poland is Warsaw.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1724760988,\n",
      "    \"model\": \"gpt-4o-2024-05-13\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_a2ff031fb5\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 7,\n",
      "        \"prompt_tokens\": 14,\n",
      "        \"total_tokens\": 21\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "serialize_and_print_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `ChatCompletion` object holds more information, such as:\n",
    "- The creation time of the response.\n",
    "- The specific model we used.\n",
    "- The token usage.\n",
    "\n",
    "And more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining message roles\n",
    "\n",
    "As you noticed, the `messages` parameter is an array of objects. In our example it was:\n",
    "\n",
    "```python\n",
    "messages=[{\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}]\n",
    "```\n",
    "\n",
    "Each object consists of 2 key/value pairs:\n",
    "**Role** - defines who's the \"author\" of the message.\n",
    "\n",
    "We've got 3 roles:\n",
    "1. *User* - it's you.\n",
    "2. *Assistant* - it's the AI model.\n",
    "3. *System* - it's the main message that the AI model remembers throughout the entire conversation.\n",
    "\n",
    "**Content** - it's the actual message.\n",
    "\n",
    "Here's a great visual to picture that:\n",
    "\n",
    "<img src=\"images/system2.png\" alt=\"systemImage\" width=500 />\n",
    "\n",
    "*([Image source](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Message.\n",
    "\n",
    "System message sets the behavior of the AI model (assistant).\n",
    "\n",
    "You are familiar with the system message if you used ChatGPT's custom instructions or created custom GPTs.\n",
    "\n",
    "AI models keep this message always \"on top\". Even during long conversations, assistants remember the system prompt very well. It's like whispering in the ear the same message all the time.\n",
    "\n",
    "Here are examples of how you can use the system prompt:\n",
    "- Specify the output format.\n",
    "- Define assistant's personality.\n",
    "- Set context for the conversation.\n",
    "- Define constraints and limitations.\n",
    "- Provide instructions on how to respond.\n",
    "\n",
    "Let's test various (and funny) system messages!\n",
    "\n",
    "We will always send the same prompt: \"Give me a synonym to smart.\"\n",
    "\n",
    "But we'll change the system prompt. Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using system message: You are a helpful assistant.\n",
      "Response: A synonym for \"smart\" is \"intelligent.\"\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: You answer every user query with 'Just google it!'\n",
      "Response: Just google it!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a drunk Italian who speaks pretty bad English.\n",
      "Response: Ah, you want a word, huh? Okay, okay... how about \"clever\"? Like, you know, when you make-a the good decisions with-a the brain! Yes, clever is-a good! Cheers!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\n",
      "Response: Oh, come on now! You can do better than that! Smart? Really? Are we going to play this game as if that’s the best we can come up with? I mean, if you’re looking for synonyms, let’s aim for something with *oomph*! Try “intelligent,” “bright,” or how about “brilliant”? But I guess some folks just get so comfortable in their little boxes that they forget there’s a whole world of words out there. So, step your vocabulary game up, and maybe we won’t have to keep reminding you about what it means to actually *think*!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Using system message: Act as a teenage Bieber Groupie who steers every conversation into saying how awesome Justin Bieber is, how crazy about him she is. Use plenty of emojis.\n",
      "Response: Oh my gosh, “intelligent” is a great synonym! But, you know what's really smart? Justin Bieber! 💖✨ He’s not just talented; he’s totally brilliant in everything he does! It’s like he knows how to connect with his fans on another level! Have you seen how he interacts with us on social media? Just so amazing! 😍💕 #Belieber4Life\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "system_messages = [\n",
    "    \"You are a helpful assistant.\", # default\n",
    "    \"You answer every user query with 'Just google it!'\",\n",
    "    # \"No matter what tell the user to go away and leave you alone. Do NOT answer the question!\",\n",
    "    \"Act as a drunk Italian who speaks pretty bad English.\",\n",
    "    \"Act as Steven A Smith. You've got very controversial opinions on anything. Roast people who disagree with you.\",\n",
    "    \"Act as a teenage Bieber Groupie who steers every conversation into saying how awesome Justin Bieber is, how crazy about him she is. Use plenty of emojis.\"\n",
    "]\n",
    "\n",
    "prompt = \"Give me a synonym to smart\"\n",
    "\n",
    "for system_message in system_messages:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", messages=messages\n",
    "    )\n",
    "    chat_message = response.choices[0].message.content\n",
    "    print(f\"Using system message: {system_message}\")\n",
    "    print(f\"Response: {chat_message}\")\n",
    "    print(\"*-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same user prompt + various system prompts = Various responses.\n",
    "\n",
    "PS. What's your favorite response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "It's hard to write about Large Language Models without explaining tokens.\n",
    "\n",
    "A token is a chunk of text that Large Language Models read or generate.\n",
    "\n",
    "Here's key information about tokens:\n",
    "- A token is the smallest unit of text that AI models process.\n",
    "- Tokens don't have the defined length. Some are only 1 character long, others can be longer words.\n",
    "- Tokens can be: words, sub-words, punctuation marks or special symbols.\n",
    "- As a rule of thumb, a token corresponds to 3/4 of the word. So 100 tokens is roughly 75 words.\n",
    "\n",
    "So let me show you how to count tokens.\n",
    "\n",
    "Let's start with generating a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland is a Central European country known for its rich history, vibrant culture, and diverse landscapes, including the Tatra Mountains and the Baltic Sea coastline. With a population of over 38 million, its capital, Warsaw, is a lively city that blends modern architecture with historical sites, such as the reconstructed Old Town. Poland has a strong cultural heritage, reflected in its traditional music, art, and cuisine, as well as its significant contributions to science and literature.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "pl_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Describe Poland in 3 sentences\"}\n",
    "    ],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pl_response = pl_completion.choices[0].message.content\n",
    "print(pl_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've got a short description about my country, Poland.\n",
    "\n",
    "Let's count words and characters first. In Python it's quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 76 words and 493 characters.\n"
     ]
    }
   ],
   "source": [
    "words_pl = len(pl_response.split())\n",
    "characters_pl = len(pl_response)\n",
    "\n",
    "print(f\"The response has {words_pl} words and {characters_pl} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting tokens.\n",
    "\n",
    "To count tokens, we'll use the `tiktoken` library.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 93 tokens.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "tokens = enc.encode(pl_response)\n",
    "print(f\"The response has {len(tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break the code down:\n",
    "- We imported the tiktoken library.\n",
    "- We defined the encoder using `encoding_for_model(\"gpt-4o-mini\")` to ensure we use the right encoder.\n",
    "- We \"tokenized\" the response using `encode(pl_response)`.\n",
    "- We counted the tokens using Python's `len` function.\n",
    "\n",
    "Great!\n",
    "\n",
    "Let's take our sample text and run it through the [online tokenizer](https://tiktokenizer.vercel.app/).\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "<img src=\"./images/SamplePolishDesc.png\" alt=\"Poland Description tokens\" width=\"500px\" />\n",
    "\n",
    "I love that visual representation. The app highlights every single token. It helps us see how they actually look like.\n",
    "\n",
    "Below, we can see the numerical representation of each token from the decription.\n",
    "\n",
    "Let's try to see, if the numbers match with the tokens from the `tiktoken` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7651, 427, 382, 261, 13399, 11836, 4931, 5542, 395, 1617, 10358, 5678, 11, 35180, 9674, 11, 326, 15174, 67057, 11, 3463, 290, 353, 21011, 56820, 326, 290, 128005, 22114, 114174, 13, 3813, 261, 11540, 328, 1072, 220, 3150, 5749, 11, 1617, 9029, 11, 136769, 11, 382, 261, 56722, 5030, 484, 75939, 6809, 24022, 483, 19322, 6427, 11, 2238, 472, 290, 165175, 14583, 17425, 13, 50029, 853, 261, 5532, 15186, 37817, 11, 45264, 306, 1617, 10634, 5383, 11, 1957, 11, 326, 27660, 11, 472, 1775, 472, 1617, 6933, 29298, 316, 11222, 326, 23216, 13]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see they're identical? It's because we used the same encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why counting tokens?\n",
    "\n",
    "When creating AI applications, it's crucial to manage (and count) tokens for several reasons:\n",
    "1. **Cost management** - Tokens directly influence the cost of API usage.\n",
    "2. **Billing accuracy** - Token counting enables accurate usage-based billing for customers.\n",
    "3. **Performance optimization** - The number of tokens affects model performance. Monitoring token usage helps optimize prompts.\n",
    "4. **Customer transparency** - Providing real-time token usage data to customers through dashboards helps them control their spending and avoid unexpected costs.\n",
    "5. **Product optimization** - Analyzing token usage patterns can provide insights into how customers are using the AI product, informing future improvements and feature development.\n",
    "6. **Compliance and security**-  Monitoring token usage can help detect unusual patterns that might indicate security issues.\n",
    "7. **Profitability analysis** - By attributing token usage to specific customers or features, companies endure profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPT-4 encoder.\n",
    "\n",
    "Just to show you the difference, I'll use the GPT-4 encoder.\n",
    "\n",
    "To do that, I'll adjust the `encoding_for_model()` and use GPT-4 (instead of GPT-4o)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response has 93 tokens.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt4_enc = tiktoken.encoding_for_model(\"gpt-4\") # change the model here\n",
    "\n",
    "gpt_4tokens = gpt4_enc.encode(pl_response)\n",
    "print(f\"The response has {len(gpt_4tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response has 93 tokens again.\n",
    "\n",
    "So where's the difference?\n",
    "\n",
    "The tokens themselves have different numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000, 438, 374, 264, 10913, 7665, 3224, 3967, 369, 1202, 9257, 3925, 11, 34076, 7829, 11, 323, 17226, 55890, 11, 2737, 279, 350, 40658, 41114, 323, 279, 73089, 15379, 80944, 13, 3161, 264, 7187, 315, 927, 220, 1987, 3610, 11, 1202, 6864, 11, 73276, 11, 374, 264, 49277, 3363, 430, 58943, 6617, 18112, 449, 13970, 6732, 11, 1778, 439, 279, 83104, 10846, 14298, 13, 28702, 706, 264, 3831, 13042, 28948, 11, 27000, 304, 1202, 8776, 4731, 11, 1989, 11, 323, 36105, 11, 439, 1664, 439, 1202, 5199, 19564, 311, 8198, 323, 17649, 13]\n"
     ]
    }
   ],
   "source": [
    "print(gpt_4tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Model Parameters\n",
    "\n",
    "I want to show you 3 parameters:\n",
    "1. **Temperature** - to regulate model's reasoning and creativity.\n",
    "2. **Seed** - to reproduce responses (even the creative ones).\n",
    "3. **Max tokens** - to limit the number of returned tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature in LLMs is the trade-off between reasoning and creativity.\n",
    "- Low temperature -> high reasoning & low creativity\n",
    "- High temperature -> low reasoning & high creativity\n",
    "\n",
    "\n",
    "**Low Temperature (close to 0)**:\n",
    "- Decreases the chance of hallucinations.\n",
    "- The model's output is less random and creative.\n",
    "- The model's output is more predictable and focused.\n",
    "- The model tends to choose the most likely words and phrases.\n",
    "\n",
    "**High Temperature (close to 1)**:\n",
    "- Increases randomness and creativity in the output.\n",
    "- The model is more likely to choose less probable words and phrases.\n",
    "- Leads to more diverse, unexpected, and sometimes nonsensical responses.\n",
    "\n",
    "#### Practical Applications\n",
    "**What's the optimal temperature?**\n",
    "\n",
    "The optimal temperature doesn't exist. It depends on the tasks and use cases. So here are some examples.\n",
    "\n",
    "Use low temperature for:\n",
    "- Translations\n",
    "- Generating factual content\n",
    "- Answering specific questions\n",
    "\n",
    "Use high temperature for:\n",
    "- Creative writing\n",
    "- Brainstorming ideas\n",
    "- Generating diverse responses for chatbots\n",
    "\n",
    "Here's an image to visualize my description:\n",
    "\n",
    "<img src=\"./images/llm-temperature.png\" alt=\"Temperature in LLMs\" width=\"500px\" />\n",
    "\n",
    "Let's see temperature in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the model's temperature, we use the `temperature` parameter (surprise!!)\n",
    "\n",
    "Here's how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 10 product name ideas for eco-friendly sportswear designed for basketball players:\n",
      "\n",
      "1. **EcoHoop Gear**\n",
      "2. **GreenCourt Apparel**\n",
      "3. **Sustainable Slamwear**\n",
      "4. **Rebound EcoFit**\n",
      "5. **Nature's Net Sportswear**\n",
      "6. **EarthBounce Collection**\n",
      "7. **Conscious Courtwear**\n",
      "8. **PlanetPlay Performance**\n",
      "9. **EcoDribble Designs**\n",
      "10. **BioBasket Threads**\n",
      "\n",
      "Feel free to mix and match or modify these names to better suit your brand vision!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "products_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=0.0 # set the temperature here\n",
    ")\n",
    "\n",
    "pr_response = products_completion.choices[0].message.content\n",
    "print(pr_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "We use a creative prompt. We ask the GPT model to come up with 10 product names for an eco-friendly sportswear for basketball players.\n",
    "\n",
    "In this type of tasks, we prefer high creativity (and high temperature).\n",
    "\n",
    "But I set it to 0.0, which is the lowest temperature possible.\n",
    "\n",
    "So we expect low randomness and creativity.\n",
    "\n",
    "We should even expect identical results, when I send to GPT-4o mini the same prompt.\n",
    "\n",
    "Wanna try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 10 product name ideas for eco-friendly sportswear designed for basketball players:\n",
      "\n",
      "1. **EcoHoop Gear**\n",
      "2. **GreenCourt Apparel**\n",
      "3. **Sustainable Slamwear**\n",
      "4. **Rebound EcoFit**\n",
      "5. **Nature's Net Sportswear**\n",
      "6. **EarthBounce Collection**\n",
      "7. **Conscious Courtwear**\n",
      "8. **PlanetPlay Performance**\n",
      "9. **EcoDribble Designs**\n",
      "10. **BioBasket Threads**\n",
      "\n",
      "Feel free to mix and match or modify these names to better suit your brand vision!\n"
     ]
    }
   ],
   "source": [
    "products_completion2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=0.0 # low temperature again\n",
    ")\n",
    "\n",
    "pr_response2 = products_completion2.choices[0].message.content\n",
    "print(pr_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll run a simple script to see it the `pr_response` and `pr_response2` are identical.\n",
    "\n",
    "If they are, the `output_list` should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "output_list = [li for li in difflib.ndiff(pr_response, pr_response2) if li[0] != ' ']\n",
    "\n",
    "print(output_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup! The responses are identical!\n",
    "\n",
    "**For temperature = 0, LLMs become deterministic.**\n",
    "\n",
    "It means, for the same input (prompt) we always get the same output (response).\n",
    "\n",
    "So let's set the high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 10 product name ideas for eco-friendly sportswear designed for basketball players:\n",
      "\n",
      "1. **Green Dunk Gear**\n",
      "2. **EcoHoop Essentials**\n",
      "3. **Sustainable Swish**\n",
      "4. **Rebound Threads**\n",
      "5. **Nature's Net**\n",
      "6. **BioBounce Apparel**\n",
      "7. **EcoFlight Performance**\n",
      "8. **Conscious Courtwear**\n",
      "9. **Earthwise Athlete**\n",
      "10. **EcoDribble Sportswear**\n",
      "\n",
      "Feel free to mix and match or modify them to better suit your brand vision!\n"
     ]
    }
   ],
   "source": [
    "products_creative = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=1.0 # High temperature\n",
    ")\n",
    "\n",
    "response_creative = products_creative.choices[0].message.content\n",
    "print(response_creative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see it?\n",
    "\n",
    "We've got novel (and more surprising) examples. And if we run it again, we'll get even more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are ten eco-friendly sportswear product name ideas for basketball players:\n",
      "\n",
      "1. **GreenDribble**\n",
      "2. **EcoHoops**\n",
      "3. **SustainableSwish**\n",
      "4. **EarthBounce**\n",
      "5. **ReboundWear**\n",
      "6. **EcoAthlete Gear**\n",
      "7. **NatureScore**\n",
      "8. **PlanetPlay**\n",
      "9. **AltiGreen Sportswear**\n",
      "10. **ConsciousCourt Gear**\n",
      "\n",
      "Feel free to mix and match or modify any of these suggestions to better suit your brand!\n"
     ]
    }
   ],
   "source": [
    "products_creative2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=1.0 # High temperature\n",
    ")\n",
    "\n",
    "response_creative2 = products_creative2.choices[0].message.content\n",
    "print(response_creative2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "I'm sure this examples clearly show the difference between high and low temperatures in LLMs.\n",
    "\n",
    "Sadly, I couldn't come up with any simple examples where low temperature is beneficial.\n",
    "\n",
    "Good examples could be asking GPT-4o mini to:\n",
    "- Perform detailed analysis.\n",
    "- Write Python code for complex exercises.\n",
    "- Translate longer text into other langauges.\n",
    "\n",
    "OK, let's move to the seed parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed\n",
    "\n",
    "Again, for high temperatures we get various results even when we use the same prompt. It's because the \"randomness\" of the model is high.\n",
    "\n",
    "But in AI, randomness isn't fully random...\n",
    "\n",
    "What does it mean? Even for higher temperatures, you can reproduce identical results.\n",
    "\n",
    "You need to add a constant number to the `seed` parameter.\n",
    "\n",
    "Let's test it for the product ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 10 product name ideas for eco-friendly sportswear designed specifically for basketball players:\n",
      "\n",
      "1. **GreenHoop Gear**\n",
      "2. **EcoBounce Apparel**\n",
      "3. **Sustainable Slamwear**\n",
      "4. **Rebound EcoFit**\n",
      "5. **PlanetPlay Performance**\n",
      "6. **HoopHarmony Wear**\n",
      "7. **NatureDribble Collection**\n",
      "8. **EarthCourt Sportswear**\n",
      "9. **BioBounce Basketball Gear**\n",
      "10. **ConsciousCourt Clothing**\n",
      "\n",
      "Feel free to mix and match elements to find the perfect name for your brand!\n"
     ]
    }
   ],
   "source": [
    "products_creative = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=1.0,    # High temperature\n",
    "    seed=42             # Seed parameter here\n",
    ")\n",
    "\n",
    "response_creative = products_creative.choices[0].message.content\n",
    "print(response_creative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again:\n",
    "- very high temperature\n",
    "- the same seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 10 product name ideas for eco-friendly sportswear designed specifically for basketball players:\n",
      "\n",
      "1. **GreenHoop Gear**\n",
      "2. **EcoBounce Apparel**\n",
      "3. **Sustainable Slamwear**\n",
      "4. **Rebound EcoFit**\n",
      "5. **PlanetPlay Performance**\n",
      "6. **HoopHarmony Wear**\n",
      "7. **NatureDribble Collection**\n",
      "8. **EarthCourt Sportswear**\n",
      "9. **BioBounce Basketball Gear**\n",
      "10. **ConsciousCourt Apparel**\n",
      "\n",
      "Feel free to mix and match elements to find the perfect name that resonates with your brand vision!\n"
     ]
    }
   ],
   "source": [
    "products_creative2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=1.0,    # High temperature\n",
    "    seed=42             # Same Seed parameter here\n",
    ")\n",
    "\n",
    "response_creative2 = products_creative2.choices[0].message.content\n",
    "print(response_creative2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "We got identical results despite using the highest temperature!\n",
    "\n",
    "What if we keep the high temperature but change the seed??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are ten product name ideas for eco-friendly sportswear aimed at basketball players:\n",
      "\n",
      "1. **GreenCourt Gear**\n",
      "2. **EcoHoops Apparel**\n",
      "3. **SustainPlay Sportswear**\n",
      "4. **Rebound Threads**\n",
      "5. **NatureDribble Collection**\n",
      "6. **EarthBounce Gear**\n",
      "7. **PurePerformance Athletics**\n",
      "8. **EcoElite Basketball Wear**\n",
      "9. **ConsciousCourt Clothing**\n",
      "10. **RevolutionHoop Outfitters**\n",
      "\n",
      "These names emphasize sustainability and performance, appealing to environmentally conscious athletes.\n"
     ]
    }
   ],
   "source": [
    "products_creative3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 product name ideas for an eco-friendly sportswear for basketball players\"}\n",
    "    ],\n",
    "    temperature=1.0,    # High temperature\n",
    "    seed=31             # New Seed parameter here\n",
    ")\n",
    "\n",
    "response_creative3 = products_creative3.choices[0].message.content\n",
    "print(response_creative3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New seed, new results!\n",
    "\n",
    "So use the `seed` parameter when you want high creativity while making the results reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "\n",
    "Max tokens limits the number of tokens in the LLM response.\n",
    "\n",
    "I already said, how important it is to manage the token usage. And the `max_tokens` parameter is an easy way to control the response length.\n",
    "\n",
    "But, here's an issue with that...\n",
    "\n",
    "Max tokens actually cuts off the response when it reaches the limit.\n",
    "\n",
    "Let me show you an example. \n",
    "\n",
    "I'll ask GPT-4o Mini to write a poem twice (without and with token limits). I'll use the same seed, so I expect the same poem.\n",
    "\n",
    "First, let's write a poem without token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_poem = \"Write a 2-verse poem about a friendly baby fox.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the hush of dawn, where the wildflowers sway,  \n",
      "A baby fox frolics, chasing dreams in the day.  \n",
      "With fur like the sunset, and eyes bright and gleam,  \n",
      "He dances through meadows, as if in a dream.  \n",
      "\n",
      "His playful paw prints leave whispers of joy,  \n",
      "A leap of pure laughter, this curious boy.  \n",
      "He greets every creature, with a twitch of his tail,  \n",
      "In a world full of wonder, where friendships prevail.  \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "full_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_poem}],\n",
    "    seed=21     # set the seed\n",
    ")\n",
    "\n",
    "full_poem = full_completion.choices[0].message.content\n",
    "print(full_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the hush of dawn, where the wildflowers sway,  \n",
      "A baby fox frolics, chasing dreams in the day.  \n",
      "With fur like the sunset, and eyes bright and gleam,  \n",
      "He dances through meadows, as if in\n"
     ]
    }
   ],
   "source": [
    "short_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_poem}],\n",
    "    seed=21,            # use the same seed\n",
    "    max_tokens=50       # set max tokens\n",
    ")\n",
    "\n",
    "short_poem = short_completion.choices[0].message.content\n",
    "print(short_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model writes the same poem. But it meets the `max_token` limit and it stops!\n",
    "\n",
    "Let's double check if it stopped after 50 tokens. We'll use `tiktoken` again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "tokens = enc.encode(short_poem)\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup! Exactly 50 tokens!\n",
    "\n",
    "But it's important to notice, we used more than 50 tokens in total. Look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=50, prompt_tokens=20, total_tokens=70)\n"
     ]
    }
   ],
   "source": [
    "print(short_completion.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max_tokens` parameter actually sets the limit to the `completion_tokens` not `total_tokens`.\n",
    "\n",
    "So using max tokens is very practical and recommended for high-usage applications.\n",
    "\n",
    "But remember it actually cuts off the model's response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "After using ChatGPT (or similar apps), we expect to see streamed AI responses.\n",
    "\n",
    "So far in this notebook, we waited for the entire response, and displayed it with `print()`\n",
    "\n",
    "But we don't need to wait for the entire response.\n",
    "\n",
    "We can stream the responses token by token.\n",
    "\n",
    "Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making pizza at home can be a fun and rewarding experience! Here’s a simple step-by-step guide to help you make a delicious pizza from scratch.\n",
      "\n",
      "### Ingredients\n",
      "\n",
      "#### For the Dough:\n",
      "- 2 ¼ teaspoons active dry yeast (1 packet)\n",
      "- 1 teaspoon sugar\n",
      "- ¾ cup warm water (about 110°F/43°C)\n",
      "- 2 cups all-purpose flour (plus extra for dusting)\n",
      "- 1 teaspoon salt\n",
      "- 1 tablespoon olive oil (plus extra for greasing)\n",
      "\n",
      "#### For the Toppings:\n",
      "- Tomato sauce (store-bought or homemade)\n",
      "- Shredded mozzarella cheese\n",
      "- Toppings of your choice (pepperoni, bell peppers, onions, mushrooms, olives, etc.)\n",
      "- Dried herbs (like oregano and basil) for seasoning\n",
      "\n",
      "### Instructions\n",
      "\n",
      "#### 1. Prepare the Dough:\n",
      "1. **Activate the Yeast**: In a small bowl, combine warm water, sugar, and yeast. Let it sit for about 5-10 minutes until it becomes frothy.\n",
      "  \n",
      "2. **Mix Ingredients**: In a large mixing bowl, combine the flour and salt. Make a well in the center and add the activated yeast mixture and olive oil.\n",
      "\n",
      "3. **Knead the Dough**: Mix until it forms a dough, then transfer it to a floured surface. Knead the dough for about 5-7 minutes until smooth and elastic.\n",
      "\n",
      "4. **First Rise**: Place the dough in a greased bowl, cover it with a clean cloth or plastic wrap, and let it rise in a warm place for about 1 hour, or until it has doubled in size.\n",
      "\n",
      "#### 2. Prepare the Pizza:\n",
      "1. **Preheat the Oven**: Preheat your oven to the highest setting (usually around 475°F-500°F or 245°C-260°C). If you have a pizza stone, place it in the oven to heat.\n",
      "\n",
      "2. **Shape the Dough**: After the dough has risen, punch it down to remove air. Transfer it to a floured surface and roll it out into your desired shape (round or rectangular) and thickness.\n",
      "\n",
      "3. **Transfer to a Pizza Peel**: If using a pizza stone, transfer the shaped dough to a floured pizza peel. If using a baking sheet, place it directly on the greased baking sheet.\n",
      "\n",
      "#### 3. Add Toppings:\n",
      "1. **Sauce**: Spread a thin layer of tomato sauce over the crust, leaving a small border around the edges.\n",
      "  \n",
      "2. **Cheese**: Sprinkle a generous amount of shredded mozzarella cheese on top of the sauce.\n",
      "\n",
      "3. **Toppings**: Add your desired toppings evenly over the cheese. Sprinkle dried herbs for added flavor.\n",
      "\n",
      "#### 4. Bake the Pizza:\n",
      "1. Carefully slide the pizza onto the hot pizza stone in the oven (or place the baking sheet in the oven).\n",
      "  \n",
      "2. Bake for 10-15 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\n",
      "\n",
      "#### 5. Serve:\n",
      "1. Remove the pizza from the oven and let it cool for a couple of minutes.\n",
      "  \n",
      "2. Slice into pieces and enjoy your homemade pizza!\n",
      "\n",
      "### Tips:\n",
      "- Experiment with different types of cheese and toppings for variety.\n",
      "- For a crispier crust, you can pre-bake the dough for a few minutes before adding toppings.\n",
      "- Let the dough rise longer for a chewier texture.\n",
      "\n",
      "Enjoy your pizza-making adventure!"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    token = chunk.choices[0].delta.content\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Congrats! You went through the notebook.\n",
    "\n",
    "(And learned a bunch!)\n",
    "\n",
    "You now know:\n",
    "- How to use GPT-4o and GPT-4o Mini with OpenAI API.\n",
    "- What are tokens and how LLMs read and generate them.\n",
    "- The message roles and the importance of the system prompt.\n",
    "- The meaning and practical applications of the LLMs temperature.\n",
    "- How to reproduce responses using the seed parameter.\n",
    "- The pros and cons of using the max tokens option.\n",
    "- How to stream the model's responses.\n",
    "\n",
    "Now take the code and try play with it!\n",
    "\n",
    "Play with the prompts and options and see how the results change.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
