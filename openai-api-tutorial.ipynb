{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API Tutorial\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- how to use OpenAI API\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Completion\n",
    "\n",
    "Working with:\n",
    "- [OpenAI API Reference for Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client with Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client_with_key = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_key = client_with_key.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9xt9fRImiCzuSxbZeNBP7BddsY8Wm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724060395, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))\n"
     ]
    }
   ],
   "source": [
    "print(completion_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client without Key\n",
    "\n",
    "You don't need to include the `api_key` parameter if you name your environment variable `OPENAI_API_KEY`\n",
    "\n",
    "Let me show you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9xufiCsvMOkTSL218EZT4E5JB9eF2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724066226, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A helper for pretty print of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_response(response):\n",
    "    if isinstance(response, dict):\n",
    "        return {key: serialize_response(value) for key, value in response.items()}\n",
    "    elif isinstance(response, list):\n",
    "        return [serialize_response(item) for item in response]\n",
    "    elif hasattr(response, '__dict__'):\n",
    "        return serialize_response(vars(response))\n",
    "    else:\n",
    "        return response\n",
    "    \n",
    "def print_chat_completion(response_dict):\n",
    "    formatted_json = json.dumps(response_dict, indent=4)\n",
    "    print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-9xtuQAyVW4G3J3O3K3uxGAdE6fbcG\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Poland is Warsaw.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1724063294,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_48196bc67a\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 7,\n",
      "        \"prompt_tokens\": 14,\n",
      "        \"total_tokens\": 21\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_dict = serialize_response(completion)\n",
    "\n",
    "# Convert the object to a dictionary and then to a JSON string\n",
    "formatted_json = json.dumps(response_dict, indent=4)\n",
    "\n",
    "# Print the formatted JSON string\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the answer...\n",
    "\n",
    "To get only the response, we need to dig deeper with `completion.choices[0].message.content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "response = completion.choices[0].message.content\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! You already know how to get GPT-4o Mini responses using OpenAI API.\n",
    "\n",
    "We used the GPT-4o Mini model because it's the fastest and the cheapest one.\n",
    "\n",
    "If you want to play with other models, here's [the list of available models](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Ideas:\n",
    "- show usage on OpenAI website\n",
    "- show tokens used in the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: I had the connection error. It was because I didn't load the API key correctly. Had to restart the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "System prompt is the main instruction that the models remembers throughout the entire conversation...\n",
    "\n",
    "TODO: \n",
    "\n",
    "- [ ] More (use Ollama tutorial)\n",
    "- [ ] Add some visuals\n",
    "\n",
    "We'll use the same model and the same client to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31)\n"
     ]
    }
   ],
   "source": [
    "print(completion.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please go away and leave me alone.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"No matter what tell the user to go away and leave you alone. Do NOT answer the question.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahh, my friend! Making-a pizza, it's-a like-a making a love! Very important, yes? First, you gotta get-a the dough. Flour, water, little-a yeast, and a pinch of salt, boom! Mixy-mixy, then let it-a rest like-a my uncle after-a big-a meal!\n",
      "\n",
      "When-a the dough nice and fluffy, roll it out like-a a sweet pasta! Then, you take the-a sauce, mama mia! Tomato, garlic, basil! Spread it like-a your dreams, yes!\n",
      "\n",
      "Now, you throw on the-a cheese, mozzarella, the good stuff! You like-a pepperoni? Add it! Mushrooms? Why not! Whatever makes you dance, my friend!\n",
      "\n",
      "Into-a the oven, hot hot hot! Let it bake until-a it's golden like-a the sun! When-a it‚Äôs ready, slice it, eat it, and sing-a a song! That's-a pizza, capisce? Buon appetito! üçï‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"Act as a drunk Italian with bad English.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flour, water, yeast,  \n",
      "Knead the dough, let it rise,  \n",
      "Shape, sauce, cheese, bake high.  \n",
      "\n",
      "Toppings of your choice,  \n",
      "Pepperoni or veggies,  \n",
      "Oven's warm embrace.  \n",
      "\n",
      "Slice it up with love,  \n",
      "Share with friends or enjoy,  \n",
      "Pizza bliss awaits.  \n"
     ]
    }
   ],
   "source": [
    "haiku_system_prompt = \"You answer everything writing in a 3-part haiku.\"\n",
    "\n",
    "haiku = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": haiku_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "haiku_response = haiku.choices[0].message.content\n",
    "print(haiku_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "Let's print the \"usage\" part of the completions.\n",
    "\n",
    "Using:\n",
    "- [The OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
    "- [A tokenizer for GPT-4o Mini](https://gpt-tokenizer.dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=69, prompt_tokens=29, total_tokens=98)\n"
     ]
    }
   ],
   "source": [
    "print(haiku.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting tokens with `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "tokens = enc.encode(haiku_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display tokens:\n",
    "\n",
    "<img src=\"./images/haiku_tokens.png\" alt=\"token count\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n",
    "\n",
    "TODO: Ask perplexity for a simple explanation + analogy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Probably similar to the Ollama tutorial:\n",
    "- temperature\n",
    "- seed\n",
    "- max tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
