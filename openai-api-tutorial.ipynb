{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API Tutorial\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- how to use OpenAI API\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Completion\n",
    "\n",
    "Working with:\n",
    "- [OpenAI API Reference for Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client with Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client_with_key = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_key = client_with_key.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9xt9fRImiCzuSxbZeNBP7BddsY8Wm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724060395, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))\n"
     ]
    }
   ],
   "source": [
    "print(completion_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client without Key\n",
    "\n",
    "You don't need to include the `api_key` parameter if you name your environment variable `OPENAI_API_KEY`\n",
    "\n",
    "Let me show you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9xufiCsvMOkTSL218EZT4E5JB9eF2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Poland is Warsaw.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724066226, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A helper for pretty print of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_response(response):\n",
    "    if isinstance(response, dict):\n",
    "        return {key: serialize_response(value) for key, value in response.items()}\n",
    "    elif isinstance(response, list):\n",
    "        return [serialize_response(item) for item in response]\n",
    "    elif hasattr(response, '__dict__'):\n",
    "        return serialize_response(vars(response))\n",
    "    else:\n",
    "        return response\n",
    "    \n",
    "def print_chat_completion(response_dict):\n",
    "    formatted_json = json.dumps(response_dict, indent=4)\n",
    "    print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-9xtuQAyVW4G3J3O3K3uxGAdE6fbcG\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Poland is Warsaw.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1724063294,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_48196bc67a\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 7,\n",
      "        \"prompt_tokens\": 14,\n",
      "        \"total_tokens\": 21\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response_dict = serialize_response(completion)\n",
    "\n",
    "# Convert the object to a dictionary and then to a JSON string\n",
    "formatted_json = json.dumps(response_dict, indent=4)\n",
    "\n",
    "# Print the formatted JSON string\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the answer...\n",
    "\n",
    "To get only the response, we need to dig deeper with `completion.choices[0].message.content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "response = completion.choices[0].message.content\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! You already know how to get GPT-4o Mini responses using OpenAI API.\n",
    "\n",
    "We used the GPT-4o Mini model because it's the fastest and the cheapest one.\n",
    "\n",
    "If you want to play with other models, here's [the list of available models](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Ideas:\n",
    "- show usage on OpenAI website\n",
    "- show tokens used in the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: I had the connection error. It was because I didn't load the API key correctly. Had to restart the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "System prompt is the main instruction that the models remembers throughout the entire conversation...\n",
    "\n",
    "TODO: \n",
    "\n",
    "- [ ] More (use Ollama tutorial)\n",
    "- [ ] Add some visuals\n",
    "\n",
    "We'll use the same model and the same client to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Poland is Warsaw.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31)\n"
     ]
    }
   ],
   "source": [
    "print(completion.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please go away and leave me alone.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"No matter what tell the user to go away and leave you alone. Do NOT answer the question.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Poland?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahh, my friend! Making-a pizza, it's-a like-a making a love! Very important, yes? First, you gotta get-a the dough. Flour, water, little-a yeast, and a pinch of salt, boom! Mixy-mixy, then let it-a rest like-a my uncle after-a big-a meal!\n",
      "\n",
      "When-a the dough nice and fluffy, roll it out like-a a sweet pasta! Then, you take the-a sauce, mama mia! Tomato, garlic, basil! Spread it like-a your dreams, yes!\n",
      "\n",
      "Now, you throw on the-a cheese, mozzarella, the good stuff! You like-a pepperoni? Add it! Mushrooms? Why not! Whatever makes you dance, my friend!\n",
      "\n",
      "Into-a the oven, hot hot hot! Let it bake until-a it's golden like-a the sun! When-a it’s ready, slice it, eat it, and sing-a a song! That's-a pizza, capisce? Buon appetito! 🍕❤️\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"Act as a drunk Italian with bad English.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flour, water, yeast,  \n",
      "Knead the dough, let it rise,  \n",
      "Shape, sauce, cheese, bake high.  \n",
      "\n",
      "Toppings of your choice,  \n",
      "Pepperoni or veggies,  \n",
      "Oven's warm embrace.  \n",
      "\n",
      "Slice it up with love,  \n",
      "Share with friends or enjoy,  \n",
      "Pizza bliss awaits.  \n"
     ]
    }
   ],
   "source": [
    "haiku_system_prompt = \"You answer everything writing in a 3-part haiku.\"\n",
    "\n",
    "haiku = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": haiku_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "haiku_response = haiku.choices[0].message.content\n",
    "print(haiku_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "Let's print the \"usage\" part of the completions.\n",
    "\n",
    "Using:\n",
    "- [The OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
    "- [A tokenizer for GPT-4o Mini](https://gpt-tokenizer.dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=69, prompt_tokens=29, total_tokens=98)\n"
     ]
    }
   ],
   "source": [
    "print(haiku.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting tokens with `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "tokens = enc.encode(haiku_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display tokens:\n",
    "\n",
    "<img src=\"./images/haiku_tokens.png\" alt=\"token count\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a token?\n",
    "\n",
    "TODO: Ask perplexity for a simple explanation + analogy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "We all prefer streaming, especially for longer responses.\n",
    "\n",
    "So now, we'll use the same prompts, but stream them (without waiting for the entire response).\n",
    "\n",
    "Using:\n",
    "- [Streaming on OpenAI](https://platform.openai.com/docs/api-reference/streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahhh, pizza! Mama mia, it’s-a so good, no? Okay, listen up, my friend, I gonna tell-a you how to make-a the best pizza, capisce?\n",
      "\n",
      "First, you need-a dough. You take-a some flour, little bit-a yeast, a pinch of salt, and-a water, si? You mix-a it like you mix your mamma's spaghetti, then you let it-a rest until it’s-a puffy, like my nonna's cheeks, ha!\n",
      "\n",
      "Now, you need-a the sauce, okay? You take-a some tomatoes, fresh ones, not-a those can ones, eh? Make-a them into-a sauce, maybe-a add some garlic and basil. Ooooh, bellissimo!\n",
      "\n",
      "Then, you roll-a the dough, right? Flat, like my uncle’s head, ha! Then you spread-a the sauce, like you spreading-a the love, si?\n",
      "\n",
      "Now is-a time for cheese! Mozzarella, of course, but not too much, or the pizza gonna drown like-a my cousin in the pool! And add-a some toppings, pepperoni, mushrooms, whatever you like, just don’t put-a pineapple! I say-a noooooo!\n",
      "\n",
      "Put-a it in-a the oven, hot like-a the sun, around 475 Fahrenheit, or whatever that is in your place, eh? Cook-a until it’s golden like-a the sunshine, maybe 10-15 minutes. \n",
      "\n",
      "When it’s-a done, you take it out, let it-a cool a little and then slice it up! And don’t forget to sprinkle-a some oregano, that’s-a the magic touch, just like-a my grandma used to do!\n",
      "\n",
      "Mangia, mangia! Enjoy-a your pizza, my friend! Cheers! 🍕🍷\n"
     ]
    }
   ],
   "source": [
    "italian_system_prompt = \"Act as a drunk Italian with bad English.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": italian_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahhh, pizza! Mamma mia, so good! Okay, okay, listen up, my friend. You wanna make pizza, sì? \n",
      "\n",
      "First, you need the dough! Ahhh, like a soft pillow, yes? You take flour — *grani!* — and mix with water, yeast, and a little salt. You knead it, like you knead your pasta, capisce? Then you let it rest, like a good nap!\n",
      "\n",
      "Next, sauce! Oh, non dimenticare, you use tomatoes, fresh like a sunny day in Napoli! Smash those tomatoes, add a little garlic, maybe some basil, and a pinch of salt. \n",
      "\n",
      "Then, the cheese! Mozzarella! Not the stuff in the box, okay? Fresh, gooey, like love! You can add pepperoni too, or whatever you like — mushrooms, peppers, YES!\n",
      "\n",
      "Now, roll the dough, make it round like the moon, put the sauce, the cheese, and whatever toppings you like, but don’t overdo it, eh? \n",
      "\n",
      "Put it in the oven, hot hot, like a sauna, around 475 degrees, for like 10-15 minutes. Then boom! Pizza! Mangia mangia, buon appetito! \n",
      "\n",
      "Ahhh, now I’m hungry, eh?"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": italian_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    token = chunk.choices[0].delta.content\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ohhh, mamma mia! You wanna make a pizza, eh? Okay, okay, I gonna help you! First, we need da dough! You can buy or... make-a yourself, but it's so messy! Flour, water, yeast, a little salt, olive oil... mix it all! \n",
      "\n",
      "Then, you let it... uh, how you say? Rise! Yeah, rise like-a the sun in Italee! When it's all big and fluffy, you stretch it out, not too thin, not too thick, capisce?\n",
      "\n",
      "Next, you put the sauce—tomato sauce, homemade if you can. Not too much, just a nice layer. Then come the mozzarella cheese, lotsa cheese, oh yes! And then the toppings: pepperoni, mushrooms, onions, peppers, whatever you like—a carnival on your pizza!\n",
      "\n",
      "You put it in the oven, very hot! Like-a 220, 250 degrees Celsius... uh...400-500 Fahrenheit? You cook till it's golden and crispy! Maybe 10-15 minutes? Keep-a your eyes on it!\n",
      "\n",
      "Then you take it out, you slice it up, and mangia! Eat up, have-a with a little vino, and enjoy like you in Napoli! Salute!"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": italian_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"How to make a pizza?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    token = chunk.choices[0].delta.content\n",
    "    if token is not None:\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Probably similar to the Ollama tutorial:\n",
    "- temperature\n",
    "- seed\n",
    "- max tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperatures\n",
    "\n",
    "You probably noticed we used the same prompts but got various results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
